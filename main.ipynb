{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "- Import\n",
    "- Read data (vocab, sentences)\n",
    "- Build model\n",
    "- Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-22T09:21:51.783827Z",
     "start_time": "2018-03-22T09:21:51.778930Z"
    }
   },
   "outputs": [],
   "source": [
    "use_cuda = True\n",
    "batch_size = 1\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-21T04:01:55.240626Z",
     "start_time": "2018-03-21T04:01:55.235314Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "# Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-22T09:21:52.524119Z",
     "start_time": "2018-03-22T09:21:51.787665Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import math\n",
    "import random\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "\n",
    "import scripts.text\n",
    "import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-22T09:21:52.541586Z",
     "start_time": "2018-03-22T09:21:52.528233Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_path = './processed-data/id.1000/'\n",
    "en_vocab_path = data_path + 'train.10k.en.vocab'\n",
    "de_vocab_path = data_path + 'train.10k.de.vocab'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-22T09:21:52.559987Z",
     "start_time": "2018-03-22T09:21:52.544961Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "# Loading vocab file ./processed-data/id.1000/train.10k.en.vocab ...\n",
      "  num words = 1000\n",
      "# Loading vocab file ./processed-data/id.1000/train.10k.de.vocab ...\n",
      "  num words = 1000\n"
     ]
    }
   ],
   "source": [
    "en_words, en_vocab, _ = scripts.text.load_vocab(en_vocab_path)\n",
    "de_words, de_vocab, _ = scripts.text.load_vocab(de_vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-22T09:21:53.002155Z",
     "start_time": "2018-03-22T09:21:52.567875Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Read train data\n",
    "en_train_sentences = []\n",
    "with open(data_path + 'train.10k.en', 'r') as f:\n",
    "    for line in f:\n",
    "        en_train_sentences.append(map(lambda x: int(x), line.split()))\n",
    "        \n",
    "de_train_sentences = []\n",
    "with open(data_path + 'train.10k.de', 'r') as f:\n",
    "    for line in f:\n",
    "        de_train_sentences.append(map(lambda x: int(x), line.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-22T09:21:53.024596Z",
     "start_time": "2018-03-22T09:21:53.006169Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Read validation data\n",
    "en_valid_sentences = []\n",
    "with open(data_path + 'valid.100.en', 'r') as f:\n",
    "    for line in f:\n",
    "        en_valid_sentences.append(map(lambda x: int(x), line.split()))\n",
    "        \n",
    "de_valid_sentences = []\n",
    "with open(data_path + 'valid.100.de', 'r') as f:\n",
    "    for line in f:\n",
    "        de_valid_sentences.append(map(lambda x: int(x), line.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Using RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-22T09:21:53.056566Z",
     "start_time": "2018-03-22T09:21:53.029234Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    \"\"\"\n",
    "        Model's encoder using RNN.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.GRU(embedding_size, hidden_size, num_layers)\n",
    "\n",
    "    def forward(self, input_sentence, hidden):\n",
    "        sentence_len = len(input_sentence)\n",
    "        \n",
    "        embedded = self.embedding(input_sentence)\n",
    "        embedded = embedded.view(sentence_len, batch_size, -1)\n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        hidden = Variable(torch.zeros(self.num_layers, batch_size, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            hidden = hidden.cuda()\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-22T09:21:53.079431Z",
     "start_time": "2018-03-22T09:21:53.061734Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    \"\"\"\n",
    "        Model's decoder using RNN.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_size, hidden_size, output_size, num_layers=1):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(output_size, embedding_size)\n",
    "        self.rnn = nn.GRU(embedding_size, hidden_size, num_layers)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input_vector, hidden):\n",
    "        output = self.embedding(input_vector).view(1, batch_size, -1)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.rnn(output, hidden)\n",
    "        output = self.log_softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        hidden = Variable(torch.zeros(self.num_layers, batch_size, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            hidden = hidden.cuda()\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Checking the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-21T06:47:13.050299Z",
     "start_time": "2018-03-21T06:47:13.043015Z"
    },
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "encoder_test = EncoderRNN(10, 10, 20, 2)\n",
    "decoder_test = DecoderRNN(10, 20, 10, 2)\n",
    "print(encoder_test)\n",
    "print(decoder_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-21T06:47:17.096257Z",
     "start_time": "2018-03-21T06:47:13.054397Z"
    },
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "encoder_hidden = encoder_test.init_hidden()\n",
    "word_input = Variable(torch.LongTensor([1, 2, 3]))\n",
    "if use_cuda:\n",
    "    encoder_test.cuda()\n",
    "    word_input = word_input.cuda()\n",
    "encoder_outputs, encoder_hidden = encoder_test(word_input, encoder_hidden)\n",
    "\n",
    "word_inputs = Variable(torch.LongTensor([1, 2, 3, 4]))\n",
    "decoder_hidden = encoder_hidden\n",
    "\n",
    "if use_cuda:\n",
    "    decoder_test.cuda()\n",
    "    word_inputs = word_inputs.cuda()\n",
    "\n",
    "for i in range(4):\n",
    "    decoder_output, decoder_hidden = decoder_test(word_inputs[i], decoder_hidden)\n",
    "    print(decoder_output)\n",
    "    print(decoder_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-22T09:21:53.088131Z",
     "start_time": "2018-03-22T09:21:53.083426Z"
    }
   },
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "clip = 5.0\n",
    "MAX_LENGTH = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-22T09:21:53.157216Z",
     "start_time": "2018-03-22T09:21:53.092133Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(input_variable, target_variable, encoder, decoder, encoder_optimizer,\n",
    "          decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    # Zero gradient\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "    loss = 0\n",
    "\n",
    "    # Get size of input and target sentences\n",
    "    input_length = input_variable.size()[0]\n",
    "    target_length = target_variable.size()[0]\n",
    "\n",
    "    # Run words through encoder\n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "\n",
    "    # Prepare input for decoder and output variables\n",
    "    decoder_input = Variable(torch.LongTensor([[de_vocab['<s>']]]))\n",
    "    decoder_hidden = encoder_hidden  # Use last hidden from the encoder\n",
    "\n",
    "    if use_cuda:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "\n",
    "    # Choose whether to use teacher forcing\n",
    "    use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: use the ground-truth target as the next input\n",
    "        for d_i in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "            loss += criterion(decoder_output, target_variable[d_i])\n",
    "            decoder_input = target_variable[d_i]\n",
    "    else:\n",
    "        # Without teacher forcing use its own predictions as the next input\n",
    "        for d_i in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden)\n",
    "#             print(decoder_output)\n",
    "            loss += criterion(decoder_output, target_variable[d_i])\n",
    "            # Pick most likely word index (highest value) from output (greedy search)\n",
    "            top_value, top_index = decoder_output.data.topk(1)\n",
    "            n_i = top_index[0][0]\n",
    "#             print(n_i)\n",
    "#             print(torch.LongTensor([n_i]))\n",
    "            decoder_input = Variable(torch.LongTensor([[n_i]])) # Chosen word is next input\n",
    "            \n",
    "            if use_cuda:\n",
    "                decoder_input = decoder_input.cuda()\n",
    "\n",
    "            # Stop at end of sentence (not necessary when using known targers)\n",
    "            if n_i == en_vocab['</s>']:\n",
    "                break\n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "#     nn.utils.clip_grad_norm(encoder.parameters(), clip)\n",
    "#     nn.utils.clip_grad_norm(decoder.parameters(), clip)\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "    \n",
    "    return loss.data[0] / target_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-22T09:21:59.230487Z",
     "start_time": "2018-03-22T09:21:53.160254Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_size = 500\n",
    "hidden_size = 500\n",
    "num_layers = 4\n",
    "dropout_p = 0.00\n",
    "\n",
    "# Initialize models\n",
    "encoder = EncoderRNN(len(en_vocab), embedding_size, hidden_size, num_layers)\n",
    "decoder = DecoderRNN(embedding_size, hidden_size, len(de_vocab), num_layers)\n",
    "\n",
    "# Move models to GPU\n",
    "if use_cuda:\n",
    "    encoder = encoder.cuda()\n",
    "    decoder = decoder.cuda()\n",
    "    \n",
    "# Initialize parameters and criterion\n",
    "# learning_rate = 0.0001\n",
    "# encoder_optimizer = torch.optim.SGD(encoder.parameters(), lr=learning_rate, momentum=0.9)\n",
    "# decoder_optimizer = torch.optim.SGD(decoder.parameters(), lr=learning_rate, momentum=0.9)\n",
    "encoder_optimizer = torch.optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-22T09:21:59.240199Z",
     "start_time": "2018-03-22T09:21:59.234337Z"
    }
   },
   "outputs": [],
   "source": [
    "# Configuring training\n",
    "num_epochs = 1\n",
    "plot_every = 100\n",
    "print_every = 100\n",
    "\n",
    "# Keep track of time elapsed and running averages\n",
    "plot_losses = []\n",
    "print_loss_total = 0 # Reset every print every\n",
    "plot_loss_total = 0 # Reset every plot every"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-22T09:22:02.999609Z",
     "start_time": "2018-03-22T09:21:59.242876Z"
    }
   },
   "outputs": [],
   "source": [
    "# Convert all sentences to Variable\n",
    "if use_cuda:\n",
    "    for i in range(len(en_train_sentences)):\n",
    "        en_train_sentences[i] = Variable(torch.LongTensor(en_train_sentences[i]).view(-1, 1)).cuda()\n",
    "        de_train_sentences[i] = Variable(torch.LongTensor(de_train_sentences[i]).view(-1, 1)).cuda()\n",
    "else:\n",
    "    for i in range(len(en_train_sentences)):\n",
    "        en_train_sentences[i] = Variable(torch.LongTensor(en_train_sentences[i]).view(-1, 1))\n",
    "        de_train_sentences[i] = Variable(torch.LongTensor(de_train_sentences[i]).view(-1, 1))\n",
    "\n",
    "if use_cuda:\n",
    "    for i in range(len(en_valid_sentences)):\n",
    "        en_valid_sentences[i] = Variable(torch.LongTensor(en_valid_sentences[i]).view(-1, 1)).cuda()\n",
    "        de_valid_sentences[i] = Variable(torch.LongTensor(de_valid_sentences[i]).view(-1, 1)).cuda()\n",
    "else:\n",
    "    for i in range(len(en_valid_sentences)):\n",
    "        en_valid_sentences[i] = Variable(torch.LongTensor(en_valid_sentences[i]).view(-1, 1))\n",
    "        de_valid_sentences[i] = Variable(torch.LongTensor(de_valid_sentences[i]).view(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-22T09:49:34.637212Z",
     "start_time": "2018-03-22T09:22:03.003471Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/1, Time: 0m 18s (- 31m 3s), Step: 100/10000, train_loss: 4.8068\n",
      "Epoch 0/1, Time: 0m 36s (- 29m 43s), Step: 200/10000, train_loss: 4.7536\n",
      "Epoch 0/1, Time: 0m 51s (- 27m 32s), Step: 300/10000, train_loss: 4.5545\n",
      "Epoch 0/1, Time: 1m 6s (- 26m 27s), Step: 400/10000, train_loss: 4.5978\n",
      "Epoch 0/1, Time: 1m 22s (- 26m 14s), Step: 500/10000, train_loss: 4.4612\n",
      "Epoch 0/1, Time: 1m 38s (- 25m 44s), Step: 600/10000, train_loss: 4.6629\n",
      "Epoch 0/1, Time: 1m 55s (- 25m 37s), Step: 700/10000, train_loss: 4.6192\n",
      "Epoch 0/1, Time: 2m 12s (- 25m 25s), Step: 800/10000, train_loss: 4.4722\n",
      "Epoch 0/1, Time: 2m 31s (- 25m 31s), Step: 900/10000, train_loss: 4.3995\n",
      "Epoch 0/1, Time: 2m 47s (- 25m 11s), Step: 1000/10000, train_loss: 4.5654\n",
      "Epoch 0/1, Time: 3m 4s (- 24m 54s), Step: 1100/10000, train_loss: 4.4284\n",
      "Epoch 0/1, Time: 3m 20s (- 24m 28s), Step: 1200/10000, train_loss: 4.4863\n",
      "Epoch 0/1, Time: 3m 36s (- 24m 10s), Step: 1300/10000, train_loss: 4.5076\n",
      "Epoch 0/1, Time: 3m 53s (- 23m 53s), Step: 1400/10000, train_loss: 4.4319\n",
      "Epoch 0/1, Time: 4m 10s (- 23m 37s), Step: 1500/10000, train_loss: 4.6171\n",
      "Epoch 0/1, Time: 4m 24s (- 23m 9s), Step: 1600/10000, train_loss: 4.3457\n",
      "Epoch 0/1, Time: 4m 40s (- 22m 47s), Step: 1700/10000, train_loss: 4.3194\n",
      "Epoch 0/1, Time: 4m 56s (- 22m 32s), Step: 1800/10000, train_loss: 4.3658\n",
      "Epoch 0/1, Time: 5m 14s (- 22m 22s), Step: 1900/10000, train_loss: 4.5033\n",
      "Epoch 0/1, Time: 5m 33s (- 22m 12s), Step: 2000/10000, train_loss: 4.4119\n",
      "Epoch 0/1, Time: 5m 50s (- 21m 57s), Step: 2100/10000, train_loss: 4.3630\n",
      "Epoch 0/1, Time: 6m 7s (- 21m 42s), Step: 2200/10000, train_loss: 4.3954\n",
      "Epoch 0/1, Time: 6m 24s (- 21m 26s), Step: 2300/10000, train_loss: 4.4767\n",
      "Epoch 0/1, Time: 6m 42s (- 21m 13s), Step: 2400/10000, train_loss: 4.3100\n",
      "Epoch 0/1, Time: 6m 58s (- 20m 56s), Step: 2500/10000, train_loss: 4.4516\n",
      "Epoch 0/1, Time: 7m 15s (- 20m 40s), Step: 2600/10000, train_loss: 4.3966\n",
      "Epoch 0/1, Time: 7m 30s (- 20m 17s), Step: 2700/10000, train_loss: 4.3108\n",
      "Epoch 0/1, Time: 7m 45s (- 19m 55s), Step: 2800/10000, train_loss: 4.3255\n",
      "Epoch 0/1, Time: 8m 1s (- 19m 38s), Step: 2900/10000, train_loss: 4.3362\n",
      "Epoch 0/1, Time: 8m 16s (- 19m 18s), Step: 3000/10000, train_loss: 4.2918\n",
      "Epoch 0/1, Time: 8m 31s (- 18m 58s), Step: 3100/10000, train_loss: 4.2734\n",
      "Epoch 0/1, Time: 8m 48s (- 18m 43s), Step: 3200/10000, train_loss: 4.4299\n",
      "Epoch 0/1, Time: 9m 4s (- 18m 25s), Step: 3300/10000, train_loss: 4.5215\n",
      "Epoch 0/1, Time: 9m 19s (- 18m 5s), Step: 3400/10000, train_loss: 4.1801\n",
      "Epoch 0/1, Time: 9m 36s (- 17m 50s), Step: 3500/10000, train_loss: 4.4662\n",
      "Epoch 0/1, Time: 9m 53s (- 17m 35s), Step: 3600/10000, train_loss: 4.4310\n",
      "Epoch 0/1, Time: 10m 8s (- 17m 15s), Step: 3700/10000, train_loss: 4.3851\n",
      "Epoch 0/1, Time: 10m 24s (- 16m 59s), Step: 3800/10000, train_loss: 4.2956\n",
      "Epoch 0/1, Time: 10m 41s (- 16m 43s), Step: 3900/10000, train_loss: 4.2702\n",
      "Epoch 0/1, Time: 10m 59s (- 16m 28s), Step: 4000/10000, train_loss: 4.3269\n",
      "Epoch 0/1, Time: 11m 14s (- 16m 11s), Step: 4100/10000, train_loss: 4.3081\n",
      "Epoch 0/1, Time: 11m 32s (- 15m 55s), Step: 4200/10000, train_loss: 4.3630\n",
      "Epoch 0/1, Time: 11m 53s (- 15m 45s), Step: 4300/10000, train_loss: 4.1953\n",
      "Epoch 0/1, Time: 12m 8s (- 15m 27s), Step: 4400/10000, train_loss: 4.2789\n",
      "Epoch 0/1, Time: 12m 27s (- 15m 13s), Step: 4500/10000, train_loss: 4.3535\n",
      "Epoch 0/1, Time: 12m 44s (- 14m 57s), Step: 4600/10000, train_loss: 4.2805\n",
      "Epoch 0/1, Time: 13m 0s (- 14m 40s), Step: 4700/10000, train_loss: 4.3831\n",
      "Epoch 0/1, Time: 13m 15s (- 14m 21s), Step: 4800/10000, train_loss: 4.3084\n",
      "Epoch 0/1, Time: 13m 32s (- 14m 5s), Step: 4900/10000, train_loss: 4.2228\n",
      "Epoch 0/1, Time: 13m 47s (- 13m 47s), Step: 5000/10000, train_loss: 4.4008\n",
      "Epoch 0/1, Time: 14m 0s (- 13m 27s), Step: 5100/10000, train_loss: 4.1844\n",
      "Epoch 0/1, Time: 14m 19s (- 13m 13s), Step: 5200/10000, train_loss: 4.4259\n",
      "Epoch 0/1, Time: 14m 35s (- 12m 56s), Step: 5300/10000, train_loss: 4.3044\n",
      "Epoch 0/1, Time: 14m 52s (- 12m 39s), Step: 5400/10000, train_loss: 4.3969\n",
      "Epoch 0/1, Time: 15m 9s (- 12m 23s), Step: 5500/10000, train_loss: 4.3550\n",
      "Epoch 0/1, Time: 15m 23s (- 12m 5s), Step: 5600/10000, train_loss: 4.4731\n",
      "Epoch 0/1, Time: 15m 39s (- 11m 49s), Step: 5700/10000, train_loss: 4.3254\n",
      "Epoch 0/1, Time: 15m 56s (- 11m 32s), Step: 5800/10000, train_loss: 4.3290\n",
      "Epoch 0/1, Time: 16m 14s (- 11m 17s), Step: 5900/10000, train_loss: 4.3530\n",
      "Epoch 0/1, Time: 16m 29s (- 10m 59s), Step: 6000/10000, train_loss: 4.4325\n",
      "Epoch 0/1, Time: 16m 47s (- 10m 44s), Step: 6100/10000, train_loss: 4.3291\n",
      "Epoch 0/1, Time: 17m 5s (- 10m 28s), Step: 6200/10000, train_loss: 4.2461\n",
      "Epoch 0/1, Time: 17m 21s (- 10m 11s), Step: 6300/10000, train_loss: 4.2873\n",
      "Epoch 0/1, Time: 17m 36s (- 9m 54s), Step: 6400/10000, train_loss: 4.3401\n",
      "Epoch 0/1, Time: 17m 50s (- 9m 36s), Step: 6500/10000, train_loss: 4.2794\n",
      "Epoch 0/1, Time: 18m 5s (- 9m 19s), Step: 6600/10000, train_loss: 4.2680\n",
      "Epoch 0/1, Time: 18m 21s (- 9m 2s), Step: 6700/10000, train_loss: 4.4118\n",
      "Epoch 0/1, Time: 18m 38s (- 8m 46s), Step: 6800/10000, train_loss: 4.2829\n",
      "Epoch 0/1, Time: 18m 56s (- 8m 30s), Step: 6900/10000, train_loss: 4.2660\n",
      "Epoch 0/1, Time: 19m 12s (- 8m 13s), Step: 7000/10000, train_loss: 4.3523\n",
      "Epoch 0/1, Time: 19m 27s (- 7m 56s), Step: 7100/10000, train_loss: 4.2200\n",
      "Epoch 0/1, Time: 19m 45s (- 7m 41s), Step: 7200/10000, train_loss: 4.0136\n",
      "Epoch 0/1, Time: 20m 1s (- 7m 24s), Step: 7300/10000, train_loss: 4.2586\n",
      "Epoch 0/1, Time: 20m 17s (- 7m 7s), Step: 7400/10000, train_loss: 4.2909\n",
      "Epoch 0/1, Time: 20m 33s (- 6m 51s), Step: 7500/10000, train_loss: 4.2098\n",
      "Epoch 0/1, Time: 20m 48s (- 6m 34s), Step: 7600/10000, train_loss: 4.2714\n",
      "Epoch 0/1, Time: 21m 5s (- 6m 18s), Step: 7700/10000, train_loss: 4.1715\n",
      "Epoch 0/1, Time: 21m 22s (- 6m 1s), Step: 7800/10000, train_loss: 4.2414\n",
      "Epoch 0/1, Time: 21m 39s (- 5m 45s), Step: 7900/10000, train_loss: 4.2204\n",
      "Epoch 0/1, Time: 21m 56s (- 5m 29s), Step: 8000/10000, train_loss: 4.2888\n",
      "Epoch 0/1, Time: 22m 12s (- 5m 12s), Step: 8100/10000, train_loss: 4.2458\n",
      "Epoch 0/1, Time: 22m 26s (- 4m 55s), Step: 8200/10000, train_loss: 4.2890\n",
      "Epoch 0/1, Time: 22m 44s (- 4m 39s), Step: 8300/10000, train_loss: 4.2397\n",
      "Epoch 0/1, Time: 23m 1s (- 4m 23s), Step: 8400/10000, train_loss: 4.2721\n",
      "Epoch 0/1, Time: 23m 18s (- 4m 6s), Step: 8500/10000, train_loss: 4.1180\n",
      "Epoch 0/1, Time: 23m 34s (- 3m 50s), Step: 8600/10000, train_loss: 4.1708\n",
      "Epoch 0/1, Time: 23m 48s (- 3m 33s), Step: 8700/10000, train_loss: 4.1220\n",
      "Epoch 0/1, Time: 24m 6s (- 3m 17s), Step: 8800/10000, train_loss: 4.2150\n",
      "Epoch 0/1, Time: 24m 22s (- 3m 0s), Step: 8900/10000, train_loss: 4.1557\n",
      "Epoch 0/1, Time: 24m 37s (- 2m 44s), Step: 9000/10000, train_loss: 4.2811\n",
      "Epoch 0/1, Time: 24m 53s (- 2m 27s), Step: 9100/10000, train_loss: 4.2001\n",
      "Epoch 0/1, Time: 25m 8s (- 2m 11s), Step: 9200/10000, train_loss: 4.2609\n",
      "Epoch 0/1, Time: 25m 22s (- 1m 54s), Step: 9300/10000, train_loss: 4.0917\n",
      "Epoch 0/1, Time: 25m 36s (- 1m 38s), Step: 9400/10000, train_loss: 4.2692\n",
      "Epoch 0/1, Time: 25m 53s (- 1m 21s), Step: 9500/10000, train_loss: 4.3069\n",
      "Epoch 0/1, Time: 26m 9s (- 1m 5s), Step: 9600/10000, train_loss: 4.0365\n",
      "Epoch 0/1, Time: 26m 24s (- 0m 49s), Step: 9700/10000, train_loss: 4.1952\n",
      "Epoch 0/1, Time: 26m 39s (- 0m 32s), Step: 9800/10000, train_loss: 4.2833\n",
      "Epoch 0/1, Time: 26m 53s (- 0m 16s), Step: 9900/10000, train_loss: 4.2069\n",
      "Epoch 0/1, Time: 27m 9s (- 0m 0s), Step: 10000/10000, train_loss: 4.1544\n",
      "Validation loss: 2.2216\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for epoch in range(0, num_epochs):\n",
    "    #start epoch\n",
    "    # Shuffle\n",
    "    indexes = np.arange(0, len(en_train_sentences))\n",
    "    np.random.shuffle(indexes)\n",
    "    step = 1\n",
    "    num_steps = math.ceil(len(en_train_sentences) / batch_size)\n",
    "    for index in indexes:\n",
    "        input_variable = en_train_sentences[index]\n",
    "        target_variable = de_train_sentences[index]\n",
    "        loss = train(input_variable, target_variable, encoder, decoder, encoder_optimizer,\n",
    "                     decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "        \n",
    "        if step == 0:\n",
    "            step += 1\n",
    "            continue\n",
    "        \n",
    "        if step % print_every == 0 or step == num_steps:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print_summary = 'Epoch %s/%s, Time: %s, Step: %d/%d, train_loss: %.4f' % (epoch, num_epochs,\n",
    "                                                                utils.time_since(start, step / num_steps),\n",
    "                                                                step,\n",
    "                                                                num_steps, print_loss_avg)\n",
    "            print(print_summary)\n",
    "        \n",
    "        if step % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total\n",
    "        step += 1\n",
    "    # end epoch\n",
    "    # evaluate on validation set\n",
    "    valid_total_loss = 0\n",
    "    for i in range(len(en_valid_sentences)):\n",
    "        input_variable = en_valid_sentences[i]\n",
    "        output_varible = de_valid_sentences[i]\n",
    "        valid_loss = train(input_variable, target_variable, encoder, decoder, encoder_optimizer,\n",
    "                     decoder_optimizer, criterion)\n",
    "        valid_total_loss += valid_loss\n",
    "    print('Validation loss: %.4f' % (valid_total_loss / len(en_valid_sentences)))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-22T09:49:34.664204Z",
     "start_time": "2018-03-22T09:49:34.641113Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(sentence, max_length=MAX_LENGTH):\n",
    "    input_variable = Variable(torch.LongTensor(scripts.text.to_id(sentence.split(), en_vocab)))\n",
    "    print(input_variable)\n",
    "    if use_cuda:\n",
    "        input_variable = input_variable.cuda()\n",
    "    \n",
    "    input_length = len(input_variable)\n",
    "    \n",
    "    encoder_hidden = encoder.init_hidden()\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "    \n",
    "    # Create starting vectors for decoder\n",
    "    decoder_input = Variable(torch.LongTensor([[de_vocab['<s>']]]))\n",
    "    decoder_hidden = encoder_hidden\n",
    "    \n",
    "    if use_cuda:\n",
    "        decoder_input = decoder_input.cuda()\n",
    "    \n",
    "    decoded_words = []\n",
    "    \n",
    "    # Run through decoder\n",
    "    for d_i in range(max_length):\n",
    "        decoder_output, decoder_hidden = decoder(\n",
    "            decoder_input, decoder_hidden)\n",
    "        # Pick most likely word index (highest value) from output (greedy search)\n",
    "        top_value, top_index = decoder_output.data.topk(1)\n",
    "        n_i = top_index[0][0]\n",
    "        print(n_i)\n",
    "        decoded_words += scripts.text.to_text([n_i], de_words)\n",
    "\n",
    "        # Stop at end of sentence (not necessary when using known targers)\n",
    "        if n_i == de_vocab['</s>']:\n",
    "            break\n",
    "\n",
    "        decoder_input = Variable(torch.LongTensor([[n_i]])) # Chosen word is next input\n",
    "\n",
    "        if use_cuda:\n",
    "            decoder_input = decoder_input.cuda()\n",
    "\n",
    "            \n",
    "    return decoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-22T09:49:34.675907Z",
     "start_time": "2018-03-22T09:49:34.667946Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_sentence(s):\n",
    "    valid_sentence = s\n",
    "    \n",
    "    output_words = evaluate(valid_sentence)\n",
    "    output_sentence = ' '.join(output_words)\n",
    "    \n",
    "    print('>', valid_sentence)\n",
    "#     print('=', pair[1])\n",
    "    print('<', output_sentence)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-22T09:49:34.868075Z",
     "start_time": "2018-03-22T09:49:34.679591Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "   0\n",
      " 499\n",
      "   9\n",
      "   0\n",
      "   6\n",
      "  60\n",
      "  11\n",
      "   9\n",
      "   0\n",
      "[torch.LongTensor of size 9]\n",
      "\n",
      "0\n",
      "0\n",
      "16\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "> i am a student and he is a teacher\n",
      "< <unk> <unk> für <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_sentence('i am a student and he is a teacher')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-22T09:49:35.067377Z",
     "start_time": "2018-03-22T09:49:34.871848Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "  0\n",
      " 11\n",
      " 91\n",
      "  0\n",
      "  6\n",
      " 75\n",
      " 47\n",
      "  0\n",
      " 11\n",
      "  0\n",
      "[torch.LongTensor of size 10]\n",
      "\n",
      "0\n",
      "0\n",
      "16\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "> luck is no excuse and who has luck is successful\n",
      "< <unk> <unk> für <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk> <unk>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_sentence('luck is no excuse and who has luck is successful')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "notify_time": "5",
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "229px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
